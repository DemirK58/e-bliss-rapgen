{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Transformers.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cemkod/e-bliss-rapgen/blob/main/e-bliss-rapgen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSjqm9isq8pp"
      },
      "source": [
        "Kullanılanacak kütüphaneleri ekleyelim.\r\n",
        "- Transformers, hazır modeller içeren ve modelleri fine-tune etmeyi kolaylaştıran bir kütüphane\r\n",
        "- PyTorch Transformer'ın arkaplanda modeli eğitmek için kullandığı machine learning kütüphanesi\r\n",
        "- ipywidgets'ı da ekrana textbox, buton gibi şeyleri çizdirmekte kullanıyoruz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-zF1_hYMWC4"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install ipywidgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlgkHu8frkvB"
      },
      "source": [
        "Eğittiğimiz modeli google drive'da saklayacağız. Bu yüzden google drive'a erişim izni vererek dosya sistemine bağlamamız gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyAETWmVTvhK",
        "outputId": "7201619f-a75a-420f-c6b3-9ab6c1f19eae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHGra08wr1PH"
      },
      "source": [
        "Görkem Göknar'ın türkçe vikipedi metinleriyle eğittiği gpt-2 modelini baz olarak alacağız. Böylece model hazırdı Türkçenin dil yapısıyla ilgili bir çok şeyi bilir halde gelecek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9wPkVrYMWC9"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"gorkemgoknar/gpt2-small-turkish\")\n",
        "# GPT-2 en fazla 1024 tokenlik bir dizi uretebiliyor\n",
        "tokenizer.model_max_length=1024 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krE6FXyZsQyD"
      },
      "source": [
        "Bu adımı çalıştırmadan önce yandaki dosyalar kısmına eğitmek için kullanacağınız corpus'u upload etmeniz gerekiyor. İsmini direk lyrics1.txt olarak bırakabilir veya aşağıda değiştirebilirsiniz. \r\n",
        "\r\n",
        "Corpush direk bir düz metin dosyası. En azından 500KB civarında olmalı iyi sonuç alabilmeniz için. Genelde ne kadar büyük olursa sonuç o kadar iyi olur ama eğitim de o kadar uzun sürer. Ben parçaları aralarına boş bir satırda ~ işaret koyarak ayırdım. Bunun ne işe yaradığını üretirken göreceksiniz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx5BXuT0MWDA"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "train_path = 'lyrics1.txt'\n",
        "test_path = 'lyrics1.txt'\n",
        "\n",
        "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
        " \n",
        "def load_dataset(train_path,test_path,tokenizer):\n",
        "    train_dataset = TextDataset(\n",
        "          tokenizer=tokenizer,\n",
        "          file_path=train_path,\n",
        "          block_size=128)\n",
        "\n",
        "    test_dataset = TextDataset(\n",
        "           tokenizer=tokenizer,\n",
        "           file_path=test_path,\n",
        "           block_size=128)\n",
        " \n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "     tokenizer=tokenizer, mlm=False,\n",
        "    )\n",
        "    return train_dataset,test_dataset,data_collator\n",
        " \n",
        "train_dataset,test_dataset,data_collator = load_dataset(train_path,test_path,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hC1ryTZs_Hk"
      },
      "source": [
        "Burada eğitim için kullanacağımız parametreleri tanımlıyoruz.  num_train_epochs parametresini istediğiniz gibi değiştirebilirsiniz. Bu eğitim sürecinde bütün corpusu sistemin kaç kere göreceğini değiştiriyor. Ne kadar yüksek olursa eğitim o kadar uzun sürer. Genelde 30-35 epochtan sonrası artık daha iyiye gitmiyor ama benim kullandığımdan daha büyük bir corpus kullanırsanız belki daha uzun süre eğitmeniz gerekebilir. \r\n",
        "\r\n",
        "Diğer tüm parametreler colab'da çalıştırmaya uygun. Eğer kendi ekran kartınızda çalıştaracaksanız, ekran kartının sahip olduğu ram'e göre per_device_train_batch_size ve per_device_eval_batch_size parametrelerini değiştirmeniz gerekebilir.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koj-IvgIMWDC"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelWithLMHead\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-trap\", #The output directory\n",
        "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
        "    num_train_epochs=5, # number of training epochs\n",
        "    #learning_rate=0.00001,\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    per_device_eval_batch_size=16,  # batch size for evaluation\n",
        "    eval_steps = 100, # Number of update steps between two evaluations.\n",
        "    logging_steps = 10,\n",
        "    save_steps=1000, # after # steps model is saved\n",
        "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
        "    prediction_loss_only=True\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18P7Zqh_t5RL"
      },
      "source": [
        "Aşağıdaki hücreyi çalıştırdığınız eğitim süreci başlayacak. Periyodik olarak Training loss değerini ekrana basıyor. Bu değer düşmeyi bıraktığında artık sistem daha fazla öğrenemiyor demektir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "id": "wM24bj0nMWDD",
        "outputId": "5f52989a-bba2-48a2-fd61-213ada98f2fe"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [290/290 05:28, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.690100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.685900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>3.654100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>3.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>3.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>3.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.605800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>3.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>3.681800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>3.551700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.622100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>3.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>3.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>3.622300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>3.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.702700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>3.648500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>3.502200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>3.537400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>3.551800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=290, training_loss=3.6144421149944437, metrics={'train_runtime': 329.558, 'train_samples_per_second': 0.88, 'total_flos': 885453942620160, 'epoch': 5.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqu3IHDOuQy5"
      },
      "source": [
        "Aşağıdaki hücre eğittiğimiz modeli google drive'a kaydetmemizi sağlıyor. Daha sonra yeniden kullanmak istediğimizde direk bunu yükleyebiliriz yeniden eğitmek yerine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8R4H7T5MWDA"
      },
      "source": [
        "model.save_pretrained(\"drive/MyDrive/Rapgen/gpt2-trap-1epoch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFzh-mCGubLr"
      },
      "source": [
        "Aşağıdaki hücreyi kullanarak da modeli test edebilirsiniz. Çalıştırdığınızda bir textbox ve buton çiziyor. Textbox'ın içine metnin nasıl başlamasını istediğiniz yazıp butona tıklayarak gerisini ürettirebilirsiniz. Ben her parçayı corpusta ~ işareti ile başlattığım için direk ~ işaretiyle üretimi başlatıyorum. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAd6LduqMWC-"
      },
      "source": [
        "from ipywidgets import widgets\n",
        "model.eval();\n",
        "# input sequence\n",
        "\n",
        "textui = widgets.Textarea(\n",
        "    value='~\\n',\n",
        "    placeholder='Type something',\n",
        "    description='String:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Click me',\n",
        "    disabled=False,\n",
        "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click me',\n",
        "    icon='check'\n",
        ")\n",
        "display(textui)\n",
        "display(button)\n",
        "\n",
        "def handle_submit(sender):\n",
        "  print(\"yes\")\n",
        "  text = textui.value\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "  # model output using Top-k sampling text generation method\n",
        "  sample_outputs = model.generate(inputs.input_ids,\n",
        "                                  pad_token_id=50256,\n",
        "                                  do_sample=True, \n",
        "                                  max_length=500, # put the token number you want\n",
        "                                  min_length=300,\n",
        "                                  top_k=40, \n",
        "                                  #top_p=0.95, \n",
        "                                  temperature=0.95,\n",
        "                                  num_return_sequences=1)\n",
        "\n",
        "  # generated sequence\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "      print(\">> Generated text {}\\n\\n{}\".format(i+1, tokenizer.decode(sample_output.tolist())))\n",
        "\n",
        "\n",
        "\n",
        "button.on_click(handle_submit)\n",
        "\n",
        "\n",
        "# >> Generated text\n",
        "#    \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}